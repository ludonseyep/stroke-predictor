{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters Explained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains the parameters chosen for both the undersampling and the hyperparameters of the RandomForest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import csv\n",
    "data = pd.read_csv('healthcare-dataset-stroke-data.csv') #importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>stroke</th>\n",
       "      <th>Never_worked</th>\n",
       "      <th>children</th>\n",
       "      <th>Private</th>\n",
       "      <th>Self-employed</th>\n",
       "      <th>Govt_job</th>\n",
       "      <th>smokes</th>\n",
       "      <th>formerly smoked</th>\n",
       "      <th>never smoked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.110327</td>\n",
       "      <td>1.422222</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.391641</td>\n",
       "      <td>0.565915</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.527778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778260</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.550821</td>\n",
       "      <td>1.177778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.629258</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685439</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5078</th>\n",
       "      <td>-0.472222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.922222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.483241</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482426</td>\n",
       "      <td>0.565915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>1.583333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.143167</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>638 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age  hypertension  heart_disease  ever_married  avg_glucose_level  \\\n",
       "0     1.166667             0              1             1           4.110327   \n",
       "1     1.000000             0              0             1           3.391641   \n",
       "2     1.527778             0              1             1           0.778260   \n",
       "3     0.666667             0              0             1           2.550821   \n",
       "4     1.500000             1              0             1           2.629258   \n",
       "...        ...           ...            ...           ...                ...   \n",
       "5075  1.250000             0              0             1           0.685439   \n",
       "5078 -0.472222             0              0             0          -0.025377   \n",
       "5091  0.944444             1              0             1           0.483241   \n",
       "5093  0.555556             1              0             1           0.482426   \n",
       "5100  1.583333             1              0             1          -0.143167   \n",
       "\n",
       "           bmi  stroke  Never_worked  children  Private  Self-employed  \\\n",
       "0     1.422222       1             0         0        1              0   \n",
       "1     0.565915       1             0         0        0              1   \n",
       "2     0.966667       1             0         0        1              0   \n",
       "3     1.177778       1             0         0        1              0   \n",
       "4     0.022222       1             0         0        0              1   \n",
       "...        ...     ...           ...       ...      ...            ...   \n",
       "5075  1.555556       0             0         0        1              0   \n",
       "5078 -0.922222       0             0         1        0              0   \n",
       "5091  0.788889       0             0         0        1              0   \n",
       "5093  0.565915       0             0         0        0              0   \n",
       "5100  0.500000       0             0         0        0              1   \n",
       "\n",
       "      Govt_job  smokes  formerly smoked  never smoked  \n",
       "0            0       0                1             0  \n",
       "1            0       0                0             1  \n",
       "2            0       0                0             1  \n",
       "3            0       1                0             0  \n",
       "4            0       0                0             1  \n",
       "...        ...     ...              ...           ...  \n",
       "5075         0       0                0             0  \n",
       "5078         0       0                0             0  \n",
       "5091         0       0                0             1  \n",
       "5093         1       1                0             0  \n",
       "5100         0       0                0             1  \n",
       "\n",
       "[638 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######Encoding and cleaning#######\n",
    "\n",
    "#making a copy to avoid damaging original data file\n",
    "data_replaced = data.copy()\n",
    "data_replaced = data_replaced.drop(columns ='id')\n",
    "    \n",
    "#calculates the means to replace NaN\n",
    "mean_values = {\n",
    "    'age': data_replaced['age'].mean(),\n",
    "    'avg_glucose_level': data_replaced['avg_glucose_level'].mean(),\n",
    "    'bmi': data_replaced['bmi'].mean(),\n",
    "}\n",
    "\n",
    "#creating the new corrected database as a copy\n",
    "data_replaced_mean = data_replaced.copy().fillna(value = mean_values) \n",
    "\n",
    "#Encoding the data with Onehotencoder: gender, work_type, Residence_type and smoking_status\n",
    "\n",
    "data_onehotencoded = data_replaced_mean.copy()\n",
    "\n",
    "def onehotencode(label, data):\n",
    "    \"\"\"takes column name and data as inputs returns the one hot encoded data\"\"\"\n",
    "    hotencode = set(data_onehotencoded[label])\n",
    "    for cls in hotencode:\n",
    "        column = cls #finds each individual value in the column\n",
    "        data[column] = data[label].apply(lambda x: 1 if x == cls else 0) #creates new columns\n",
    "    del data[label]#eliminates original columns\n",
    "    return data\n",
    "\n",
    "#executes one hot encoding over the non numerical attributes\n",
    "\n",
    "for i in ['gender','work_type','Residence_type','smoking_status']:\n",
    "    data_onehotencoded = onehotencode(i,data_onehotencoded)\n",
    "\n",
    "#replaces values in ever married by 1 (Yes) and 0 (No)\n",
    "ever_married = set(data_onehotencoded['ever_married'])\n",
    "data_onehotencoded['ever_married'] = data_onehotencoded['ever_married'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "#deleting columns that are not relevant to the problem. \n",
    "for i in ['Rural', 'Urban', 'Other','Male','Female','Unknown']:\n",
    "    del data_onehotencoded[i]\n",
    "\n",
    "#######Scaling#######\n",
    "\n",
    "data_robust = data_onehotencoded.copy()\n",
    "\n",
    "def robust(label: str, data):\n",
    "    \"\"\"Standardises the data using robust scaler\"\"\"\n",
    "    rob = set(data_robust[label])\n",
    "    q1_value = np.quantile(data_robust[label], 0.25) #calculates first quartile\n",
    "    q3_value = np.quantile(data_robust[label], 0.75) #calculates third quartile\n",
    "    diff = q3_value - q1_value #substracts the two quartiles\n",
    "    #assigns the robust standardised values to the columns\n",
    "    data_robust[label] = data_robust[label].apply(lambda x: (x - q1_value) / diff)\n",
    "    return data_robust\n",
    "\n",
    "#executes feature scaling over the non binary numerical data \n",
    "\n",
    "for i in ['age','bmi','avg_glucose_level']:\n",
    "    data_robust = robust(i,data_robust)\n",
    "\n",
    "#the non stroke proportion in this dataset is non representative of the UK population where 1/6 of the inhabitants suffer...\n",
    "#...a stroke in their lifetime. \n",
    "#See https://www.gov.uk/government/news/new-figures-show-larger-proportion-of-strokes-in-the-middle-aged\n",
    "#To compensate the effect of the data bias, 92% of the stroke data is removed (non stoke individuals). This...\n",
    "#...corresponds to a random undersampling approach. This was chosen over over-sampling as it required less data processing...\n",
    "#...and improved computational efficiency. \n",
    "data_final = data_robust.drop(data_robust[data_robust['stroke'] == 0].sample(frac=0.92).index)\n",
    "\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data splitting (see previous notebook for explanations)\n",
    "X_columns = list(data_final.columns)\n",
    "training_fraction: float = 0.8\n",
    "y_column: str = 'stroke'\n",
    "X_columns.remove(y_column)\n",
    "n_rows = int(training_fraction * len(data_final))\n",
    "\n",
    "X = data_final[X_columns].to_numpy()\n",
    "y = data_final[y_column].to_numpy()\n",
    "         \n",
    "\n",
    "# get X_train and X_test\n",
    "X_train = X[:n_rows]\n",
    "X_test = X[n_rows:]\n",
    "        \n",
    "       \n",
    "y_train = y[:n_rows]\n",
    "y_test = y[n_rows:] \n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size =0.2, random_state = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.67      0.76        78\n",
      "           1       0.62      0.86      0.72        50\n",
      "\n",
      "    accuracy                           0.74       128\n",
      "   macro avg       0.75      0.76      0.74       128\n",
      "weighted avg       0.78      0.74      0.74       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = RandomForestClassifier(n_estimators= 1800, class_weight = \"balanced\", min_samples_split= 10, min_samples_leaf= 2,max_features= 'log2',max_depth= 90)\n",
    "classifier.fit( X_train,y_train)\n",
    "y_pred = classifier.predict(X_test).astype(int)\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "#fig, ax = plt.subplots()\n",
    "names = ['True Negatives','False Positives','False Negatives','True Positives']\n",
    "counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                confusion_matrix.flatten()]\n",
    "percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     confusion_matrix.flatten()/np.sum(confusion_matrix)]\n",
    "labels = [f\"{n}\\n{c}\\n{p}\" for n, c, p in\n",
    "          zip(names,counts,percentages)]\n",
    "labels1 = np.asarray(labels).reshape(2,2)\n",
    "plt.title('Confusion Matrix')\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "#annot_kws = {\"ha\": 'center',\"va\": 'center'}\n",
    "sns.heatmap(confusion_matrix, annot=labels1, fmt='', cmap='Greens',cbar=False)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Undersampling justification\n",
    "\n",
    "We can see in the classification report and confusion matrix that the score for predicting if people are going to have a stroke or not is high. This is what matters the most as we are more interested in correctly predicting if a person will have a stroke rather than not have a stroke. \n",
    "Undersampling allowed to achieve higher scores overall for the stroke prediction and was decisive in choosing the parameters of the Classifier. \n",
    "\n",
    "Moreover, we also tested the model using oversampling.Oversampling might be better at first sight as it prevents deleting additional information provided by the dataset by simply generating new positive strokes cases instead of deleting non stroke cases. \n",
    "\n",
    "However, this method gives us a very low score on predictions of people who will have a stroke. Furthermore, oversampling also gives us higher number of false negatives. This is very dangerous as it predicts that people will not have a stroke when in reality they have it.\n",
    "\n",
    "We chose undersampling because it gives us the best predictions and low false negatives all the while reducing computational effort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10, 20, 30,\n",
       "                                                              40],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The randomised search function will search the parameters through 5 fold cross validation and \n",
    "#...100 iterations to end up with the best parameters.\n",
    "Classifier = RandomForestClassifier().fit(X_train, y_train)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100,None],\n",
    "\n",
    "'max_features': ['auto', 'sqrt' ],\n",
    "\n",
    "'min_samples_leaf': [1, 2, 4],\n",
    "\n",
    "'min_samples_split': [2, 5, 10, 20, 30, 40],\n",
    "\n",
    "'n_estimators' : [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "\n",
    "}\n",
    "#choosing parameters to investigate \n",
    "\n",
    "Classifier_random= RandomizedSearchCV(estimator =  Classifier, param_distributions = param_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "Classifier_random.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1800,\n",
       " 'min_samples_split': 30,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 100}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classifier_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hypeparameters choices explained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters in Random Forest that we have studied in lectures are max_depth, max_features, min_samples_leaf, min_samples_split and n_estimators. \n",
    "We used RandomizedSearchCV to tune the hyperparameters and find their best values that give the highest score. In order to do so, we have to insert a range of values for each hyperparameter in the code, and it will run random combinations between them. \n",
    "\n",
    "For **N_estimators** we have chosen a high range: from '200' to '2000'. As n_estimators give the number of trees in the forest, higher number of trees give you better performance but makes your code slower. The higher the number of n_estimators, the better the score will be. That is why the code chose 1800 especially as the dataset is highly unbalanced. We want to maximise the number of tress in the forest to prevent inaccurate classification due to the bias of the non stroke data. \n",
    "\n",
    "For **max_depth**, we selected a range between '10' and '100' and 'None' ( this will go as deep as the value of min_samples_split). We want to try out large numbers because the deeper it goes, the more it captures information about our specific dataset which presents a large number of categories which will influence the final output. However, a high number of max_depth will have high variance and cause overfitting. As we have a high number of n_estimators, 90 as our max_depth value is a good estimate.\n",
    "\n",
    "**Max_features** is usually 'auto' or 'sqrt. Sqrt turned out to be a very good value especially for Classifiers. Looking for the best split implies looking for a large number of variables to consider for an effective split as numerous parameters in the dataset will influence each other as they are corrolated (for example age and hypertension).Furthermore choosing sqrt will prevent having exactly the same tress in the entire random Forest. \n",
    "\n",
    "We chose a small range, from 1 to 4, for **min_samples_leaf** because high numbers can cause underfitting. The minimum number obtained (2) corresponded to the minimum number of points that need to be considered to split into a new region. This allowed to reduce the presence of a high number of outlying additional data that would result in overdividing the whole sample space due to the presence of numerous datapoints and attributes to be considered. \n",
    "\n",
    "Similarly, **min_samples_split** is used to control overfitting and too high values can cause underfitting. Because of this, we chose a range between 2 and 40. If the min_sample_split = 5, and there are 9 samples at the internal node than the split is allowed. If the min_samples_leaf = 4 the split will only be allowed if the split results in 4 leafs. It would not allow a split into 1 sample and 7 samples.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "03bf1f64736ce3b8a7b9e615b7bd915f49b40bff207b89aa6580967feb0bf095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
