{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Computing Challenge 2021`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reducing the dataset \n",
    "\n",
    "Using our critical thinking, we decided that ID and residence type attributes for example are unlikely to influence the patient probability of having a stroke. This allowed to reduce the amount of noise created by the additional data and improve the predictivity of the to be chosen model. \n",
    "\n",
    "Important strokes factors include : \n",
    "age, high cholesterol and obesity (bmi), diabetes (average glucose level), smoking (smoking status, hypertension).  \n",
    "\n",
    "Factors that may have an effect on the likeliness of a stroke were still included to evaluate their potential impact on the predictivity of the chosen model (marital status, work type, intense stress,hypertension, and employment type). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import sklearn.metrics\n",
    "import statistics\n",
    "import csv\n",
    "\n",
    "data = pd.read_csv('healthcare-dataset-stroke-data.csv') #importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data processing\n",
    "\n",
    "2.1 Removing NaN values\n",
    "\n",
    "NaN values were replaced using the mean of the associated attribute (no interference with the data distribution). \n",
    "\n",
    "2.2 One hot encoder \n",
    "\n",
    "Gave numerical binary values to multiclasses categories (non numerical entries) such as : smoking status, ever married, gender. It was preferred over label encoding to prevent any hierarchical ordering of the values. Additional categories were created, the ones non representative of the whole data set, in particular Other (gender) was removed from the dataset. \n",
    "\n",
    "2.3 Feature scaling \n",
    "\n",
    "Standardised values to prevent skewing of the data and minimising the influence of outliers. The robust scaler scales features using statistics that are robust to outliers. It removes the median and scales the data in the range between the 25th percentile and 75th percentile (Q1 and Q3). This proved to be particularly efficient on the bmi attribute which exhibits a skewed distribution towards 0 (bmi near 0 and 100 are unlikely) and a noticeable difference between the mean and the median. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>stroke</th>\n",
       "      <th>Never_worked</th>\n",
       "      <th>Private</th>\n",
       "      <th>Self-employed</th>\n",
       "      <th>Govt_job</th>\n",
       "      <th>children</th>\n",
       "      <th>formerly smoked</th>\n",
       "      <th>smokes</th>\n",
       "      <th>never smoked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.110327</td>\n",
       "      <td>1.422222</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.391641</td>\n",
       "      <td>0.565915</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.527778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778260</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.550821</td>\n",
       "      <td>1.177778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.629258</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685439</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5078</th>\n",
       "      <td>-0.472222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.922222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>1.083333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>0.027778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.640657</td>\n",
       "      <td>-0.311111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>1.194444</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.391776</td>\n",
       "      <td>1.888889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>638 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age  hypertension  heart_disease  ever_married  avg_glucose_level  \\\n",
       "0     1.166667             0              1             1           4.110327   \n",
       "1     1.000000             0              0             1           3.391641   \n",
       "2     1.527778             0              1             1           0.778260   \n",
       "3     0.666667             0              0             1           2.550821   \n",
       "4     1.500000             1              0             1           2.629258   \n",
       "...        ...           ...            ...           ...                ...   \n",
       "5075  1.250000             0              0             1           0.685439   \n",
       "5078 -0.472222             0              0             0          -0.025377   \n",
       "5088  1.083333             1              0             1          -0.009635   \n",
       "5090  0.027778             0              0             0           0.640657   \n",
       "5097  1.194444             0              0             1           0.391776   \n",
       "\n",
       "           bmi  stroke  Never_worked  Private  Self-employed  Govt_job  \\\n",
       "0     1.422222       1             0        1              0         0   \n",
       "1     0.565915       1             0        0              1         0   \n",
       "2     0.966667       1             0        1              0         0   \n",
       "3     1.177778       1             0        1              0         0   \n",
       "4     0.022222       1             0        0              1         0   \n",
       "...        ...     ...           ...      ...            ...       ...   \n",
       "5075  1.555556       0             0        1              0         0   \n",
       "5078 -0.922222       0             0        0              0         0   \n",
       "5088  0.711111       0             0        0              1         0   \n",
       "5090 -0.311111       0             0        0              0         1   \n",
       "5097  1.888889       0             0        0              1         0   \n",
       "\n",
       "      children  formerly smoked  smokes  never smoked  \n",
       "0            0                1       0             0  \n",
       "1            0                0       0             1  \n",
       "2            0                0       0             1  \n",
       "3            0                0       1             0  \n",
       "4            0                0       0             1  \n",
       "...        ...              ...     ...           ...  \n",
       "5075         0                0       0             0  \n",
       "5078         1                0       0             0  \n",
       "5088         0                0       0             0  \n",
       "5090         0                0       1             0  \n",
       "5097         0                0       0             0  \n",
       "\n",
       "[638 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######Encoding and cleaning#######\n",
    "\n",
    "#making a copy to avoid damaging original data file\n",
    "data_replaced = data.copy()\n",
    "data_replaced = data_replaced.drop(columns ='id')\n",
    "    \n",
    "#calculates the means to replace NaN\n",
    "mean_values = {\n",
    "    'age': data_replaced['age'].mean(),\n",
    "    'avg_glucose_level': data_replaced['avg_glucose_level'].mean(),\n",
    "    'bmi': data_replaced['bmi'].mean(),\n",
    "}\n",
    "\n",
    "#creating the new corrected database as a copy\n",
    "data_replaced_mean = data_replaced.copy().fillna(value = mean_values) \n",
    "\n",
    "#Encoding the data with Onehotencoder: gender, work_type, Residence_type and smoking_status\n",
    "\n",
    "data_onehotencoded = data_replaced_mean.copy()\n",
    "\n",
    "def onehotencode(label, data):\n",
    "    \"\"\"takes column name and data as inputs returns the one hot encoded data\"\"\"\n",
    "    hotencode = set(data_onehotencoded[label])\n",
    "    for cls in hotencode:\n",
    "        column = cls #finds each individual value in the column\n",
    "        data[column] = data[label].apply(lambda x: 1 if x == cls else 0) #creates new columns\n",
    "    del data[label]#eliminates original columns\n",
    "    return data\n",
    "\n",
    "#executes one hot encoding over the non numerical attributes\n",
    "\n",
    "for i in ['gender','work_type','Residence_type','smoking_status']:\n",
    "    data_onehotencoded = onehotencode(i,data_onehotencoded)\n",
    "\n",
    "#replaces values in ever married by 1 (Yes) and 0 (No)\n",
    "ever_married = set(data_onehotencoded['ever_married'])\n",
    "data_onehotencoded['ever_married'] = data_onehotencoded['ever_married'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "#deleting columns that are not relevant to the problem. \n",
    "for i in ['Rural', 'Urban', 'Other','Male','Female','Unknown']:\n",
    "    del data_onehotencoded[i]\n",
    "\n",
    "#######Scaling#######\n",
    "\n",
    "data_robust = data_onehotencoded.copy()\n",
    "\n",
    "def robust(label: str, data):\n",
    "    \"\"\"Standardises the data using robust scaler\"\"\"\n",
    "    rob = set(data_robust[label])\n",
    "    q1_value = np.quantile(data_robust[label], 0.25) #calculates first quartile\n",
    "    q3_value = np.quantile(data_robust[label], 0.75) #calculates third quartile\n",
    "    diff = q3_value - q1_value #substracts the two quartiles\n",
    "    #assigns the robust standardised values to the columns\n",
    "    data_robust[label] = data_robust[label].apply(lambda x: (x - q1_value) / diff)\n",
    "    return data_robust\n",
    "\n",
    "#executes feature scaling over the non binary numerical data \n",
    "\n",
    "for i in ['age','bmi','avg_glucose_level']:\n",
    "    data_robust = robust(i,data_robust)\n",
    "\n",
    "#the non stroke proportion in this dataset is non representative of the UK population where 1/6 of the inhabitants suffer...\n",
    "#...a stroke in their lifetime. \n",
    "#See https://www.gov.uk/government/news/new-figures-show-larger-proportion-of-strokes-in-the-middle-aged\n",
    "#To compensate the effect of the data bias, 92% of the stroke data is removed. This...\n",
    "#...corresponds to a random undersampling approach. This was chosen over over-sampling as it required less data processing...\n",
    "#...and improved computational efficiency (see more justification details in Notebook 2) . \n",
    "data_final = data_robust.drop(data_robust[data_robust['stroke'] == 0].sample(frac=0.92).index)\n",
    "\n",
    "data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forest Classifier \n",
    "\n",
    "We chose Random Forest because it is one of the non-parametric classifiers that gives the highest accuracy (Over 90%). The model works so well because of the low correlation between some of the attributes (bmi and age for example) ,  the trees protect each other from their individual errors. While some trees may be wrong, many other trees will be right, so as a group, the trees are able to move in the correct direction. Some advantages of Random Forest against other classifiers include: \n",
    "- It is insensitive to noise or overtraining, which shows the ability in dealing with our unbalanced data.\n",
    "- Trees can accurately divide the data based on Categorical Variables ( not linear regression).\n",
    "\t\n",
    "With the selected feature scaling, this model proved to be a good candidate compared to other potential modelling techniques such as Logistic Regression or KNN. \n",
    "\n",
    "To evaluate the importance of each attribute, the recall_micro cross validation scores were studied over each iteration. Recall_micro was chosen to count the total number of true positives, false negatives and false positives. This proved to be a good metric to use, as the most dangerous prediction would be to falsely diagnose that a patient will not have a stroke (false negative).\n",
    "\n",
    "10 cross validations folds were iterated to prevent overfitting of the data and obtain enough datapoints for the final interactive graph. Furthermore, k (nb of cross validation folds) needs not to be increased too much as both computational efficiency and number of datapoints in a single training set will be too low. \n",
    "\n",
    "Training fraction was chosen to be 80% in order to obtain the best possible model that will fit our data and prevent important variation of the obtained score. In this scenario, a greater training data quantity will give a better estimate of the stroke output however it must not be increased above this level (ie 90-10). This is because a 10% test set is too little to evaluate the final performance of the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier Class that includes functions to return predicted data and its associated cross validation recall_micro...\n",
    "#...score when the input column is removed from the dataset. If the score decreases a lot, the attribute has a...\n",
    "#...important impact on the accuracy of our classifier predictions. \n",
    "\n",
    "\n",
    "class RandomForest(RandomForestClassifier):\n",
    "    \"\"\"final class, takes a dataframe as an input, the chosen hyperparameters and the column to be removed \"\"\"\n",
    "    def __init__(self, _JB: pd.DataFrame, hyperparameters,removed : list = [], training_fraction: float = 0.8, y_column: str = 'stroke'):\n",
    "        \n",
    "        #Initialisation \n",
    "        self.data_final = data_final.copy() #copy the data to prevent modifying the orginal set \n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "    \n",
    "        \n",
    "          # do splitting and remove specified attributes\n",
    "        X_columns = list(data_final.columns)\n",
    "        X_columns.remove(y_column)\n",
    "        for i in removed:\n",
    "            X_columns.remove (i)\n",
    "        n_rows = int(training_fraction * len(self.data_final))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "          # allocate X and y to self.X and self.y and transforming the sets into numpy arrays \n",
    "        self.X = self.data_final[X_columns].to_numpy()\n",
    "        self.y = self.data_final[y_column].to_numpy()\n",
    "         \n",
    "        \n",
    "        \n",
    "\n",
    "          # get X_train and X_test\n",
    "        self.X_train = self.X[:n_rows]\n",
    "        self.X_test = self.X[n_rows:]\n",
    "        \n",
    "        #shuffle the data\n",
    "        \n",
    "        \n",
    "        # get y_train and y_test\n",
    "        self.y_train = self.y[:n_rows]\n",
    "        self.y_test = self.y[n_rows:]  \n",
    "        \n",
    "        #Shuffle the data to prevent the absence of positive stroke cases in the test set which would inflate...\n",
    "        #...the obtained results. A consistent shuffle is used across all folds of 1000.  \n",
    "        self.X_train,self.X_test,self.y_train,self.y_test=train_test_split(self.X,self.y,test_size =0.2, random_state = 1000)\n",
    "        \n",
    "        #fit and score \n",
    "    \n",
    "        \n",
    "    def get_predicted(self) -> float :\n",
    "        classifier = self.hyperparameters.fit(self.X_train, self.y_train)\n",
    "       \n",
    "        # get predicted data and allocate to self.y_pred\n",
    "        self.y_pred = classifier.predict(self.X_test)\n",
    "        return self.y_pred\n",
    "    \n",
    "    \n",
    "    def cross_validate(self) -> list :\n",
    "        classifier = self.hyperparameters.fit(self.X_train, self.y_train)\n",
    "        cv_results = cross_validate(classifier, self.X_train, self.y_train, cv= 10, scoring = 'recall_micro')\n",
    "        cv_results2 = cv_results['test_score']\n",
    "  \n",
    "        \n",
    "        return cv_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "age [0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
      " 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0\n",
      " 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1]\n",
      "hypertension [0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "heart_disease [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "ever_married [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "avg_glucose_level [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "bmi [0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0\n",
      " 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1]\n",
      "Never_worked [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "Private [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "Self-employed [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "Govt_job [0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "children [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "formerly smoked [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "smokes [0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n",
      "never smoked [0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#loops the Class removing each time a different attribute and outputs the associated cross validation score at each...\n",
    "#.. iteration, gives access to the associated predicted values of the stroke attribute\n",
    "model = {}\n",
    "columns = list(data_final.columns)\n",
    "columns.remove('stroke')\n",
    "model['everything'] = RandomForest(data_final, RandomForestClassifier(n_estimators= 1800, class_weight = \"balanced\", min_samples_split= 10, min_samples_leaf= 2,max_features= 'sqrt',max_depth= 90))\n",
    "Prediction_list = []\n",
    "for i in columns:\n",
    "\tmodel[i] = RandomForest(data_final, RandomForestClassifier(n_estimators= 1800,class_weight = \"balanced\", min_samples_split= 10, min_samples_leaf= 2,max_features= 'sqrt',max_depth= 90) , [i])\n",
    "\n",
    "\n",
    "for i in model: \n",
    "    Prediction = model[i].get_predicted ()#updates the values for each column \n",
    "    Prediction_list.append(Prediction)\n",
    "    print (i,Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything [0.74509804 0.66666667 0.78431373 0.7254902  0.64705882 0.7254902\n",
      " 0.84313725 0.80392157 0.76470588 0.80392157]\n",
      "age [0.70588235 0.68627451 0.68627451 0.62745098 0.56862745 0.7254902\n",
      " 0.70588235 0.7254902  0.68627451 0.7254902 ]\n",
      "hypertension [0.78431373 0.66666667 0.78431373 0.70588235 0.66666667 0.70588235\n",
      " 0.82352941 0.74509804 0.76470588 0.82352941]\n",
      "heart_disease [0.76470588 0.64705882 0.78431373 0.70588235 0.68627451 0.7254902\n",
      " 0.82352941 0.80392157 0.76470588 0.78431373]\n",
      "ever_married [0.74509804 0.66666667 0.78431373 0.7254902  0.66666667 0.7254902\n",
      " 0.84313725 0.78431373 0.76470588 0.80392157]\n",
      "avg_glucose_level [0.80392157 0.70588235 0.76470588 0.64705882 0.66666667 0.7254902\n",
      " 0.84313725 0.74509804 0.74509804 0.8627451 ]\n",
      "bmi [0.74509804 0.66666667 0.80392157 0.70588235 0.64705882 0.70588235\n",
      " 0.8627451  0.74509804 0.74509804 0.82352941]\n",
      "Never_worked [0.74509804 0.64705882 0.78431373 0.74509804 0.64705882 0.7254902\n",
      " 0.84313725 0.78431373 0.76470588 0.82352941]\n",
      "Private [0.74509804 0.64705882 0.78431373 0.7254902  0.64705882 0.7254902\n",
      " 0.84313725 0.76470588 0.78431373 0.82352941]\n",
      "Self-employed [0.76470588 0.68627451 0.78431373 0.74509804 0.64705882 0.74509804\n",
      " 0.84313725 0.76470588 0.76470588 0.82352941]\n",
      "Govt_job [0.74509804 0.64705882 0.78431373 0.7254902  0.64705882 0.7254902\n",
      " 0.84313725 0.76470588 0.76470588 0.80392157]\n",
      "children [0.74509804 0.64705882 0.78431373 0.7254902  0.64705882 0.7254902\n",
      " 0.84313725 0.78431373 0.76470588 0.80392157]\n",
      "formerly smoked [0.76470588 0.66666667 0.78431373 0.7254902  0.66666667 0.7254902\n",
      " 0.84313725 0.78431373 0.78431373 0.80392157]\n",
      "smokes [0.74509804 0.66666667 0.76470588 0.7254902  0.64705882 0.76470588\n",
      " 0.84313725 0.78431373 0.76470588 0.80392157]\n",
      "never smoked [0.76470588 0.64705882 0.82352941 0.68627451 0.64705882 0.74509804\n",
      " 0.84313725 0.76470588 0.74509804 0.82352941]\n"
     ]
    }
   ],
   "source": [
    "#this time loops the class to access the associated Recall scores when each attribute is removed\n",
    "Recallscore_list = [] #converts the result into a list that will then be uploaded into a csv file \n",
    "for i in model:\n",
    "    Recallscore = model[i].cross_validate()\n",
    "    Recallscore_list.append(Recallscore)\n",
    "    print (i,Recallscore)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploads the obtained data into a csv file that will then be used for plotting the graph\n",
    "Score_evr = Recallscore_list[0]\n",
    "Score_age = Recallscore_list[1]\n",
    "Score_hyp = Recallscore_list[2]\n",
    "Score_hrt = Recallscore_list[3]\n",
    "Score_mrr = Recallscore_list[4]\n",
    "Score_glc = Recallscore_list[5]\n",
    "Score_bmi = Recallscore_list[6]\n",
    "Score_slf = Recallscore_list[7]\n",
    "Score_prv = Recallscore_list[8]\n",
    "Score_gvt = Recallscore_list[9]\n",
    "Score_nvw = Recallscore_list[10]\n",
    "Score_cld = Recallscore_list[11]\n",
    "Score_nvs = Recallscore_list[12]\n",
    "Score_fms = Recallscore_list[13]\n",
    "Score_smk = Recallscore_list[14]\n",
    "\n",
    "\n",
    "with open('data_graph.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Score\", \"Fold\", \"Attributes\", \"mean\" ])\n",
    "    for i in range (10):\n",
    "        writer.writerow([Score_evr[i], i+1, \"everything\", (statistics.mean(Score_evr))])\n",
    "        writer.writerow([Score_age[i], i+1, \"age\", (statistics.mean(Score_age))])\n",
    "        writer.writerow([Score_hyp[i], i+1, \"Hypertension\", (statistics.mean(Score_hyp))])\n",
    "        writer.writerow([Score_hrt[i], i+1, \"heart_disease\", (statistics.mean(Score_hrt))])\n",
    "        writer.writerow([Score_mrr[i], i+1, \"ever_married\", (statistics.mean(Score_mrr))])\n",
    "        writer.writerow([Score_glc[i], i+1, \"avg_glucose_level\",(statistics.mean(Score_glc))])\n",
    "        writer.writerow([Score_bmi[i], i+1, \"bmi\", (statistics.mean(Score_bmi))])\n",
    "        writer.writerow([Score_gvt[i], i+1, \"Govt job\", (statistics.mean(Score_gvt))])\n",
    "        writer.writerow([Score_cld[i], i+1, \"children\", (statistics.mean(Score_cld))])\n",
    "        writer.writerow([Score_slf[i], i+1, \"Self employed\", (statistics.mean(Score_slf))])\n",
    "        writer.writerow([Score_nvw[i], i+1, \"Never_worked\", (statistics.mean(Score_nvw))])\n",
    "        writer.writerow([Score_prv[i], i+1, \"private\", (statistics.mean(Score_prv))])\n",
    "        writer.writerow([Score_smk[i], i+1, \"smokes\", (statistics.mean(Score_smk))])\n",
    "        writer.writerow([Score_nvs[i], i+1, \"never smoked\",(statistics.mean(Score_nvs))])\n",
    "        writer.writerow([Score_fms[i], i+1, \"formerly smoked\", (statistics.mean(Score_fms))])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "df377712657eaab1668bc5cf7816089a252e3ad8c70351d49d6df707be075d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
